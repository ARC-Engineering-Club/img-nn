{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook explains Michael Nielsen's implementation of a simple, fully connected neural network.\n",
    "\n",
    "\n",
    "<br>\n",
    "A fully connected neural network works fairly well for recognizing grayscale images with low variation, such as pictures of digits and characters. For images that have greater variation, a convolutional neural network (CNN) is required. However, a convolutional network is simply an extension of a fully connected network, so it is necessary to understand how a more basic network works before moving on to CNNs.\n",
    "<br>\n",
    "\n",
    "Before reading this code, you should be fairly confident with reading and understanding Python syntax (or familiar enough in programming to figure it out). You should also be familiar with object-oriented concepts such as classes and objects. If you have any questions, post them in the ARC Engineering Club Discord or message/email a club officer. \n",
    "<br>\n",
    "\n",
    "For a more general and intuitive understanding of what the neural network is doing, it will be helpful to refer to <a href=\"https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\"><b>this</b></a> playlist of videos by 3Blue1Brown. <b> I strongly recommend watching this playlist as it will make understanding the code implementation of the network much easier </b>.\n",
    "<br>\n",
    "\n",
    "So let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the libraries that we will need for the neural network.\n",
    "<br>\n",
    "We will only be using two external packages, <i>random</i> and <i>numpy</i>.\n",
    "<ul>\n",
    "    <li><b>random</b> - This package implements pseudo-random number generators for various distributions.</li>\n",
    "    <li><b>numpy</b> - This package implements optimized linear algebra operations and data structures.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to the class header and constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have created a class, Network, that gets instantiated with a list called sizes. The list ``sizes`` contains the number of neurons in the respective layers of the network.  For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron.\n",
    "<br><br>\n",
    "An example instantiation of such a neural net would look like this: ``net = Network([2,3,1])``\n",
    "<br><br>\n",
    "To get a better feel for what is happening and what ``sizes`` represents, here is a visual representation of this example neural network.\n",
    "<br>\n",
    "<img src=\"https://gyazo.com/04f8ada656c619df79fd63d869c26d6b.png\" width=\"700\">\n",
    "<br>\n",
    "As you can see, the first layer contains 2 neurons, the second layer has 3 neurons, and the last layer has 1 neuron. ``sizes`` is just a list of the sizes for each layer. ``sizes[0]`` would give the size of the input layer, ``sizes[1]`` the size of the next, and so on up to the output layer.\n",
    "<br><br>\n",
    "``num_layers`` is a data member that represents the number of layers in the neural network. With our example, ``num_layers = 3``. The number of elements in the ``sizes`` list should be equivalent to the number of layers.\n",
    "<br><br>\n",
    "The conceptual explanation for what biases and weights are and why we use them is sort of lengthy, so watch the playlist I linked earlier, and I will just explain the code.\n",
    "<br><br>\n",
    "The ``biases`` data member is a list of lists that is instantiated using a Python list comprehension. The comprehension makes use of the np.random.randn(shape) function, which creates a matrix based on a provided shape (list of dimensions) and populates the matrix with random values (because it does not matter how the biases are initialized as they get set later via back-propagation). This list comprehension creates a list of randomly populated vectors for each layer in the neural network (excluding the input layer, hence ``sizes[1:]``) and gives each vector the same number of elements as the layer in the neural network the vector is supposed to refer to.\n",
    "<br><br>\n",
    "For a more concrete example, in our sample neural neural network the ``biases`` data member might be instatiated as:\n",
    "<br><br>$$(\\begin{bmatrix}0.334234\\\\1.123432\\\\2.675733\\end{bmatrix},\\begin{bmatrix}0.716982\\\\\\end{bmatrix})$$<br><br> where the numbers inside the vectors are random values. Referring back to the original picture, there is a single bias associated with each node except for the nodes in the first layer.<br><br>\n",
    "The weights list comprehension (``self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]``) is slightly trickier to understand. Because each neuron is a linear combination of the neurons in the previous layer, for each neuron we would need a list that is the size of the # of neurons in the previous layer. We also need a unique set of these weights for each neuron in the current layer. So for each layer we have a list of lists (2D array/matrix). The ``weights`` data member of the Network class is a list of these matrices for each layer (excluding our first input layer). The randn function is used to create a matrix of shape (# of elements in current layer, #elements in previous layer)<br><br>\n",
    "Again, for a more concrete example, in the sample neural network the ``weights`` data member might be instantiated as: <br><br>\n",
    "$$(\\begin{bmatrix}4.6199977 & 0.7580295\\\\ 3.4202695 & 1.8598127\\\\ 2.1413864 & 2.5474264\\end{bmatrix}, \\begin{bmatrix}0.624555 & 0.70114 & 3.198781\\end{bmatrix})$$\n",
    "<br><br>\n",
    "where the number of rows corresponds to the number of neurons in the current layer (again, the input layer is excluded) and the number of columns corresponds to the number of neurons in the previous layer.\n",
    "<br><br>\n",
    "These four variables: ``size``, ``num_layers``, ``biases``, and ``weights`` give the general structure for a neural network. Now we will move forward to the actual mechanisms in which the neural network processes and learns from data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function governs the mechanical action of prediction (or attempted prediction) in a fully connected neural \n",
    "network. If the weights and biases are correctly set, this process of repeatedly applying dot product, vector \n",
    "addition, and the sigmoid function will yield an ideal result (correct prediction). The parameter ``a`` signifies the data passed into the neural network (in our case an image matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data) \n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function dictates how the neural network \"learns\" from training data and how the performance of the neural network is evaluated. There is quite a bit to unpack here so I will begin with some definitions of potentially unfamiliar terms:\n",
    "<br>\n",
    "<ul>\n",
    "<li>``epochs`` - An \"epoch\" is a single pass through all of the training data provided for the neural network. Oftentimes, data gets reshuffled and passed to the network multiple times for higher accuracy, leading to multiple epochs.\n",
    "</li>\n",
    "<br>\n",
    "<li>``eta`` - eta is the \"learning rate\" of the function that can make it approach a minimum more rapidly at the risk of skipping it entirely. It's simply a constant that gets multiplied to the updates to make the algorithm descend faster or more slowly.\n",
    "</li>\n",
    "<br>\n",
    "<li>``mini_batch_size`` - In general, there are three types of ways to process data for machine learning: batch, minibatch, and online. These methods determine how the backpropagation algorithm (again, 3blue1brown explains this so watch those videos) is applied to the weights and biases in the neural network. \n",
    "<ul>\n",
    "<li>In <b>batch learning</b>, backpropagation is applied to every member in the training dataset. All of the updates accrued by the backpropagation algorithm are summed together, and then after this summation occurs, the model is updated. \n",
    "</li>\n",
    "<li>In <b>online learning</b> the backpropagation algorithm is applied to each individual training sample, and the model is updated directly after. \n",
    "</li>\n",
    "<li><b>Minibatch learning</b> is a middle ground between online learning and batch learning. In minibatch learning, the dataset is broken up and then processed as a series of smaller \"batches\", where the updates are summed in a minibatch, and then the model is changed after a minibatch is processed. There are different advantages and disadvantages to these different types of learning, but all you have to know is that for this specific implementation of a neural network we are making use of minibatch learning. \n",
    "</li>\n",
    "<li><b>The </b>``mini_batch_size`` <b>parameter determines how large we want the minibatches to be.</b>\n",
    "</li>\n",
    "</ul>\n",
    "</ul>\n",
    "    \n",
    "<br>\n",
    "This function basically says: If we have training data, For each epoch in the total number of epochs, randomly shuffle the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
